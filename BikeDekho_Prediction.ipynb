{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrWmneLjprtOydkFERGLMW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthik2533/BikeDekho_Analysis_Prediction/blob/main/BikeDekho_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjM1-K--B-3y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "sJTys_L8CBtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"BikeIndia.csv\")"
      ],
      "metadata": {
        "id": "IqtQ4toGCFb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding the detailed info of the dataset\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "0mv8qKjRCysP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "L24TCHPND_9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "pdVCEM9JEMXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find the sum of all null values in dataset\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "DCayAfY5EToS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find the sum of all duplicate values in dataset\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "ZPxtxqkOE6QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the dteday to Datetime format\n",
        "df['dteday'] = pd.to_datetime(df['dteday'])\n",
        "\n",
        "#find the duplicates and remove from the dataset\n",
        "duplicate_rows = df[df.duplicated()]\n",
        "if not duplicate_rows.empty:\n",
        "  print(\"Duplicate rows detected. Removing them\")\n",
        "  df.drop_duplicates(inplace=True)\n",
        "else:\n",
        "  print(\"No Duplicate rows found.\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cYGVJbelFFKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#savnig the cleaned dataset seperately\n",
        "df.to_csv(\"BikeIndia_Cleaned_Dataset.csv\",index=False)\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "i-_VfNqOGQh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "P__bAxZ9GlVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the cleaned dataset\n",
        "df = pd.read_csv(\"BikeIndia_Cleaned_Dataset.csv\")\n",
        "\n",
        "#removing the redundant columns\n",
        "df.drop(columns = ['instant'], inplace = True)\n",
        "\n",
        "#display the remaining column\n",
        "print(\"Remaing columns removing unwanted columns:\", df.columns)\n",
        "\n"
      ],
      "metadata": {
        "id": "iYR-MKh3I0pV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Exploratory Data Analysis\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Visualize numerical features\n",
        "numerical_features = ['temp','atemp','hum','windspeed','casual','registered','cnt']\n",
        "sns.pairplot(df[numerical_features])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IG0BxIeoJuS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize Categorical features\n",
        "\n",
        "categorical_features=['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday',\n",
        "       'weathersit']\n",
        "for feature in categorical_features:\n",
        "    sns.countplot(x=feature, data=df)\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Q1tplUX2LVXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = df.corr()\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(correlation_matrix,annot=True, cmap = 'coolwarm',fmt='.2f',linewidth=.5)\n",
        "plt.title(\"Correlation Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sXZugMkZSOcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature rescaling\n",
        "- This is done to ensure that all the feautres are on a single scale as there will be many different range of values in the dataset.\n",
        "- Here we will be using min-max scalling"
      ],
      "metadata": {
        "id": "YUjkxSgl1q75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature rescaling\n",
        "#This is done to ensure that all the feautres are on a single scale as there will be many different range of values in the dataset\n",
        "#here we will be using min-max scalling\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#we have to scale numerical features\n",
        "numerical_features = ['temp','atemp','hum','windspeed','casual','registered','cnt']\n",
        "\n",
        "#we have to intialize the scaler\n",
        "Scaler = MinMaxScaler()\n",
        "\n",
        "#Rescaling the numerical features\n",
        "df[numerical_features]=Scaler.fit_transform(df[numerical_features])\n",
        "\n",
        "print(df[numerical_features])"
      ],
      "metadata": {
        "id": "On_HA3kiTZqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Implementation\n",
        "Here we will start by simple linear regression model"
      ],
      "metadata": {
        "id": "BtNijtTX1hAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "#Simple Linear Regression\n",
        "#select the feature with highest correlation with the dependent variable\n",
        "max_corr_feature = 'temp'\n",
        "\n",
        "X_simple = df[[max_corr_feature]]\n",
        "y = df['cnt']\n",
        "\n",
        "#split the data to training sets\n",
        "X_simple_train, X_simple_test, y_train, y_test = train_test_split(X_simple,y,test_size=0.2,random_state=42)\n",
        "\n",
        "#Initialize and fit the model\n",
        "simple_lr = LinearRegression()\n",
        "simple_lr.fit(X_simple_train,y_train)\n",
        "\n",
        "#predict on the testing set\n",
        "y_pred_simple = simple_lr.predict(X_simple_test)\n",
        "print(y_pred_simple)\n",
        "\n",
        "#Evaluate the model\n",
        "mse_simple = mean_squared_error(y_test, y_pred_simple)\n",
        "rmse_simple = np.sqrt(mse_simple)\n",
        "r2_simple = r2_score(y_test, y_pred_simple)\n",
        "print(\"Simple Linear Regression MSE: \",mse_simple)\n",
        "\n"
      ],
      "metadata": {
        "id": "f7xaSW4wWKrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Linear Regression\n",
        "Here we have to exclude few features which have multicollinearity with other independent features"
      ],
      "metadata": {
        "id": "csz5ggHT1WL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Multiple Linear Regression\n",
        "#Here we have to exclude few features which have multicollinearity with other independent features\n",
        "X_multiple = df.drop(columns=['atemp','registered','casual','cnt','dteday'])\n",
        "y=df['cnt']\n",
        "\n",
        "#splitting data into train and testing sets\n",
        "X_multiple_train,X_multiple_test, y_train, y_test = train_test_split(X_multiple, y, test_size = 0.2, random_state=42)\n",
        "\n",
        "#fit the Linear Regression\n",
        "multiple_lr=LinearRegression()\n",
        "multiple_lr.fit(X_multiple_train,y_train)\n",
        "\n",
        "#Predict on the testing set\n",
        "y_pred_multiple = multiple_lr.predict(X_multiple_test)\n",
        "print(y_pred_multiple)\n",
        "\n",
        "#Evaluate the model\n",
        "mse_multiple = mean_squared_error(y_test, y_pred_multiple)\n",
        "print(\"Multiple Linear Regression MSE: \",mse_multiple)\n"
      ],
      "metadata": {
        "id": "8GL7brmDZwyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Matrics:\n",
        "- MSE is caluculated above and it measures the avg squared difference between the predicted values and actual values.\n",
        "- MSE provides model's accuracy\n",
        "-Some Evaluation matrics we use are MAE, RMSE, R2, Adjusted R-squared, MPE, MAPE, you can calculate using scikit learn"
      ],
      "metadata": {
        "id": "4nZbl3EI1HWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "import numpy as np\n",
        "\n",
        "#calculate rmse\n",
        "rmse_multiple = np.sqrt(mean_squared_error(y_test,y_pred_multiple))\n",
        "\n",
        "#calculate R_squared\n",
        "r2_multiple = r2_score(y_test,y_pred_multiple)\n",
        "\n",
        "print(\"Simple Linear Regression:\")\n",
        "print(\"MSE:\", mse_simple)\n",
        "print(\"RMSE:\", rmse_simple)\n",
        "print(\"R-squared:\", r2_simple)\n",
        "print(\"\\nMultiple Linear Regression:\")\n",
        "print(\"MSE:\", mse_multiple)\n",
        "print(\"RMSE:\", rmse_multiple)\n",
        "print(\"R-squared:\", r2_multiple)"
      ],
      "metadata": {
        "id": "Y_WwXqGpt3Jf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
